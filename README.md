# CYBER: A General Robotic Operation System for Embodied AI

![Show Data](docs/imgs/showdata.png)

The development of world models in robotics has long been a cornerstone of advanced research, with most approaches relying heavily on vast, platform-specific datasets. These datasets, while valuable, often limit scalability and generalization to different robotic platforms, restricting their broader applicability.

In contrast, **CYBER** approaches world modeling from a "first principles" perspective, drawing inspiration from how humans naturally acquire skills through experience and interaction with their environment. **CYBER** is the first general Robotic Operational System designed to adapt to both tele-operated manipulation and human operation data, enabling robots to learn and predict across a wide range of tasks and environments. It build with a <u>Physical World Model</u>, a cross-embodied <u>Visual-Language Action Model</u> (VLA), a <u>Perception Model</u>, a <u>Memory Model</u>, and a <u>Control Model</u> to help robots learn, predict, and memroy across various tasks and embodiments.

At the same time, **CYBER** also provide millions of human operation datasets and baseline models over HuggingFace ğŸ¤— to enhance embodied learning, and experimental evalaution tool box to help researchers to test and evaluate their models in both simulation and real world.

---

## ğŸŒŸ Key Features

- **ğŸ› ï¸ Modular**: Built with a modular architecture, allowing flexibility in various environments.
- **ğŸ“Š Data-Driven**: Leverages millions of human operation datasets to enhance embodied learning.
- **ğŸ“ˆ Scalable**: Scales across different robotic platforms, adapting to new environments and tasks.
- **ğŸ”§ Customizable**: Allows for customization and fine-tuning to meet specific requirements.
- **ğŸ“š Extensible**: Supports the addition of new modules and functionalities, enhancing capabilities.
- **ğŸ“¦ Open Source**: Open-source and freely available, fostering collaboration and innovation.
- **ğŸ”¬ Experimental**: Supports experimentation and testing, enabling continuous improvement.
---

## ğŸ› ï¸ Modular Components

**CYBER** is built with a modular architecture, allowing for flexibility and customization. Here are the key components:

- [**ğŸŒ World Model**](docs/tutorial/world.md): Learns from physical interactions to understand and predict the environment.
- [**ğŸ¬ Action Model**](docs/tutorial/action.md): Learns from actions and interactions to perform tasks and navigate.
- [**ğŸ‘ï¸ Perception Model**](docs/tutorial/preception.md): Processes sensory inputs to perceive and interpret surroundings.
- [**ğŸ§  Memory Model**](docs/tutorial/memory.md): Utilizes past experiences to inform current decisions.
- [**ğŸ® Control Model**](docs/tutorial/control.md): Manages control inputs for movement and interaction.

## âš™ï¸ Setup

### Pre-requisites

You will need Anaconda installed on your machine. If you don't have it installed, you can follow the installation instructions [here](https://docs.anaconda.com/anaconda/install/linux/).

### Installation

You can run the following commands to install CYBER:

```bash
bash scripts/build.sh
```

Alternatively, you can install it manually by following the steps below:

1. **Create a clean conda environment:**

        conda create -n cyber python=3.10 && conda activate cyber

2. **Install PyTorch and torchvision:**

        conda install pytorch==2.3.0 torchvision==0.18.0 cudatoolkit=11.1 -c pytorch -c nvidia

3. **Install the CYBER package:**

        pip install -e .

## ğŸ¤— Hugging Face Integration

**CYBER** leverages the power of Hugging Face for model sharing and collaboration. You can easily access and use our models through the Hugging Face platform.

### Available Data

Currently, four tasks are avaible for download:

- ğŸ¤— [Pipette](https://huggingface.co/datasets/cyberorigin/cyber_pipette)
- ğŸ¤— [Take Item](https://huggingface.co/datasets/cyberorigin/cyber_take_the_item)
- ğŸ¤— [Twist Tube](https://huggingface.co/datasets/cyberorigin/cyber_twist_the_tube)
- ğŸ¤— [Fold Towels](https://huggingface.co/datasets/cyberorigin/cyber_fold_towels)

### Available Models

Our pretrained models will be released on Hugging Face soon:

- [Cyber-World-Large](https://huggingface.co/cyberorigin/cyber-xlarge)
- [Cyber-World-Base](https://huggingface.co/cyberorigin/cyber-large)
- [Cyber-World-Small](https://huggingface.co/cyberorigin/cyber-base)

### Using the Models

To use our models in your project, you can install the `transformers` library and load the models as follows:

```python
from transformers import AutoModel, AutoTokenizer

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("cyberorigin/cyber-base")
model = AutoModel.from_pretrained("cyberorigin/cyber-base")

# Example usage
inputs = tokenizer("Hello, world!", return_tensors="pt")
outputs = model(**inputs)
```

For more details, please refer to the [Hugging Face documentation](https://huggingface.co/docs/transformers/index).


## ğŸ•¹ï¸ Usage

Please refer to the [experiments](experiments/README.md) for more details on data downloading and model training.

---

## ğŸ’¾ File Structure

```plaintext
â”œâ”€â”€ ...
â”œâ”€â”€ docs                   # documentation files and figures 
â”œâ”€â”€ docker                 # docker files for containerization
â”œâ”€â”€ examples               # example code snippets
â”œâ”€â”€ tests                  # test cases and scripts
â”œâ”€â”€ scripts                # scripts for setup and utilities
â”œâ”€â”€ experiments            # model implementation and details
â”‚   â”œâ”€â”€ configs            # model configurations
â”‚   â”œâ”€â”€ models             # model training and evaluation scripts
â”‚   â”œâ”€â”€ notebooks          # sample notebooks
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cyber                  # compression, model training, and dataset source code
â”‚   â”œâ”€â”€ dataset            # dataset processing and loading
â”‚   â”œâ”€â”€ utils              # utility functions
â”‚   â””â”€â”€ models             # model definitions and architectures
â”‚       â”œâ”€â”€ action         # visual language action model
â”‚       â”œâ”€â”€ control        # robot platform control model
â”‚       â”œâ”€â”€ memory         # lifelong memory model
â”‚       â”œâ”€â”€ perception     # perception and scene understanding model
â”‚       â”œâ”€â”€ world          # physical world model
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
```

### ğŸ“• References

Magvit2 and GENIE adapted from [1xGPT challege](https://github.com/1x-technologies/1xgpt)
1X Technologies. (2024). 1X World Model Challenge (Version 1.1) [Data set]

- [MetaSLAM Github](https://github.com/MetaSLAM)
- [MetaSLAM website](https://metaslam.github.io/)

```bibtex
@inproceedings{wang2024hpt,
author    = {Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He},
title     = {Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers},
booktitle = {Neurips},
year      = {2024}
}
```

```bibtex
@inproceedings{wang2024hpt,
author    = {Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He},
title     = {Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers},
booktitle = {Neurips},
year      = {2024}
}
```

## Contact
If you have technical questions, please open a GitHub issue. For business development or other collaboration inquiries, feel free to contact us through email ğŸ“§ (<contact@cyberorigin.ai>). Enjoy! ğŸ‰
