# Cyber

**Cyber** represents a model implementation that seamlessly integrates state-of-the-art (SOTA) world models with proposed **CyberOrigin Dataset**, pushing the boundaries of artificial intelligence and machine learning.

Follow this document to train the models using our readily-availabe data or adapt your data for training.

## CyberOrigin Dataset
Our data includes information from home services, the logistics industry, and laboratory scenarios.
For more details, please refer to our [Offical Data Website](https://cyberorigin2077.github.io/)

* **Format & Description**</br>
Currently, the dataset contains image tokens generated by Magvit2. For more information, please refer to the dataset card on [Huggingface](https://huggingface.co/datasets/cyberorigin/CyberDataset).

* **Download the Dataset**</br>
The dataset is currently available on Huggingface.

    - ðŸ¤— [Pipette](https://huggingface.co/datasets/cyberorigin/cyber_pipette)
    - ðŸ¤— [Take Item](https://huggingface.co/datasets/cyberorigin/cyber_take_the_item)
    - ðŸ¤— [Twist Tube](https://huggingface.co/datasets/cyberorigin/cyber_twist_the_tube)
    - ðŸ¤— [Fold Towels](https://huggingface.co/datasets/cyberorigin/cyber_fold_towels)

You can download the dataset using the following command:
```bash
bash ../scripts/download_dataset.sh
```

* **Visualize the Dataset**</br>
You can visualize the dataset using this [notebook](notebooks/visualize_data.ipynb).
Make sure to install the jupyter before running the notebook. `pip install jupyter notebook`


## Quick Start for CyberOrigin dataset
### Download the Dataset
```bash
bash ../scripts/download_dataset.sh
```
### Model Training & Evaluation
```
python models/world/train_world.py --train-config-path configs/models/world/world_model.yaml --model-config-path configs/models/world/MagVIT_Genie.yaml
```
*Note: The model will train on the default configuration provided.*

## Model configuration and hyperparameters
### GENIE
The code is adapted from [1x's implementation](https://github.com/1x-technologies/1xgpt) of [GENIE](https://arxiv.org/abs/2402.15391). The model is based on an ST-transformer architecture that predicts the next frame given the previous frames.

**Model parameters tuning**</br>
The detailed configuration file is provided in the `configs/models/world` folder. 
```
{
    "num_layers": 32, // number of ST-transformer blocks
    "num_heads": 8, // number of heads in multi-head attention
    "d_model": 256, // dimension of the model latent
    "T": 16, // number of frames in the input sequence
    "S": 256, // number of tokens in the input sequence S=16x16
    "image_vocab_size": 262144, // codebook size for the image tokens
    "use_mup": false, // whether to use MUP
    "num_factored_vocabs": 2, // number of factored vocabularies
    "qkv_bias": false, // whether to use bias in qkv projection
    "proj_bias": true, // whether to use bias in projection
    "attn_drop": 0, // dropout rate in attention
    "qk_norm": false, // whether to normalize qk
    "mlp_ratio": 4, // ratio of hidden size to model latent size in MLP
    "mlp_drop": 0, // dropout rate in MLP
    "mlp_bias": true // whether to use bias in MLP
}
```
It is recommended to only modify the first three parameters to adjust model size.

**Training parameters tuning**</br>
Please refer to the help message for hyperparameter descriptions
```bash
python models/world/train.py -h
```

### Magvit2
Code is modified from [1XGPT](https://github.com/1x-technologies/1xgpt) and [Open-MAGVIT2](https://github.com/TencentARC/Open-MAGVIT2) but removed unnecessary files and code.
